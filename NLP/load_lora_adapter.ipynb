{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dshome/anaconda3/envs/auto_gptq_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer, GenerationConfig, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "from peft import PeftConfig, PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка и сохранение на диск базовой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = 'models/'\n",
    "adapter_path = 'models/'\n",
    "tockenizer_path = 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"IlyaGusev/saiga2_7b_lora\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "# template_path = '/home/dshome/Projects/rulm/self_instruct/internal_prompts/saiga_v2.json'\n",
    "# config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# base_model_config = AutoConfig.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     config.base_model_name_or_path,\n",
    "#     torch_dtype=base_model_config.torch_dtype,\n",
    "#     load_in_8bit=True,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# base_model.save_pretrained(base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем базовую модель. \n",
    "# Загружаем Адаптер.\n",
    "# Загружаем токенайзер."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование работы адаптера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tockenizer_path, use_fast=False) #, padding_side='left')\n",
    "# generation_config = GenerationConfig.from_pretrained(model_name, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = PeftConfig.from_pretrained(base_model_path)\n",
    "base_model_config = AutoConfig.from_pretrained(base_model_path)\n",
    "torch_dtype = base_model_config.torch_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        llm_int8_threshold=6.0,\n",
    "        llm_int8_has_fp16_weight=False,\n",
    "        bnb_4bit_compute_dtype=torch_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    ),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(base_model_path,\n",
    "                                                     do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` attribute will be overwritten with the one you passed to `from_pretrained`.\n"
     ]
    }
   ],
   "source": [
    "# config = PeftConfig.from_pretrained(adapter_path)\n",
    "# base_model_config = AutoConfig.from_pretrained(base_model_path)\n",
    "\n",
    "# torch_dtype = base_model_config.torch_dtype\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    load_in_8bit=True,\n",
    "    # quantization_config=quantization_config, # tесли не хотим 4-bit.\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapter_path,\n",
    "    adapter_name=\"adapter1_name\",\n",
    "    torch_dtype=torch_dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (adapter1_name): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (adapter1_name): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (adapter1_name): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (adapter1_name): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (adapter1_name): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (adapter1_name): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (adapter1_name): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (adapter1_name): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (adapter1_name): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (adapter1_name): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (adapter1_name): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (adapter1_name): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adapter1_name\n"
     ]
    }
   ],
   "source": [
    "print(model.active_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выполнить предсказание на модели.\n",
    "\n",
    "# Посмотреть скрипты для загрузки модели и всех данных со своего google-drive.\n",
    "# Надо еще посомотереть как зделать chackpoint на свой google-drive. \n",
    "# Чтобы не терять данные модели.\n",
    "# Это в случае, если модель не влезал на мою видеокарту.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message =  \"Как приготовить борщ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = f\"<s>system\\n{system_prompt}</s>\\n\" + \\\n",
    "    f\"<s>user\\n{user_message}</s>\\n\" + \\\n",
    "    f\"<s>bot\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenizer(s,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k: v.to(device) for k, v in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настроить параметры генерации текста.\n",
    "output_ids = model.generate(**data, do_sample=True, temperature=0.9, max_new_tokens=512, top_p=0.95,\n",
    "                             top_k=5, repetition_penalty=1.03, no_repeat_ngram_size=2, generation_config=generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Борщ - это классический русский суп. Для его приготовления вам понадобится:\\n\\n- 1,5 кг красного или красно-желтого картофеля\\n  \\n  - 500 гр. куриного мяса\\n    - косточка, измельченная до пюре\\n    \\n      - лук, перец, соль, свежие травы (по вкусу)\\n        - масло\\n        \\nДля примечания: при использовании карри или кориандра, необходимо добавить 2 столовые ложки кокосового масла.\\n1. Сначала необязательно нарежьте мясо на куски и обжарьte на сковороде до золотистого цвета. Оставите на огне, чтобы мясные жир и соединился с маслом.  В другой миску разбейте карротины и натрите в соли, перцем и свежей зеленью. Добавьем в маслину. Если вы хотите придать супу кисло-соленый вставку, додайте небольшое количество соевого соуса. Затем перемешай, посыпь солнечными яйцами, и дополните соленой водкой. Положите смесь на среднюю плиту. Возьмите маленькую кастрюлю, положи в неё 3-4 стакана воды и закройте крышкой, придерживая её с помощью крючка или крутящейся ручкой для супа. Выполнять это нужно на медленном огоньке, так как борщу нужен длительный процесс готовки. Когда масса начинает кипить, уменьшите огнень. Убедитесь, что кастарина не перегревается, а супок не загустеет. Как только супык закипит, оставь его на 40 минут и дай ему настоя'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Переписать.\n",
    "outputs = []\n",
    "for sample_output_ids, sample_input_ids in zip(output_ids, data[\"input_ids\"]):\n",
    "    sample_output_ids = sample_output_ids[len(sample_input_ids):]\n",
    "    sample_output = tokenizer.decode(sample_output_ids, skip_special_tokens=True)\n",
    "    sample_output = sample_output.replace(\"</s>\", \"\").strip()\n",
    "    outputs.append(sample_output)\n",
    "outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Борщ — национальный украинский суп, который можно приготовлять различными способами в зависимости от местных традиций или вкусовых предпочтений. Вот несколько популярных рецептов:\\n\\n1. Основной реcipe: нарежьте картофель, морковь, лук, чеснок и кольца помидоров, затем обжарь их на оливковом масле. Добавьte воду, уксус и бульон, и варите в течение 20-30 минут до готовности. В конце добавляйте сливочное масло, зеленый луковичий перец и зелени (укроп или петрушку), и подавай.\\n2. Украинская борщу: используй для начинки крутокоренники, курицу, говядину, конину или баранины, которые нужно нашинковать, обжарить на сковороде и дополнительно обсушить. Затем, следует применять основной прием, но с использованием мяса вместо картин.  Это более сложный и насыщенный реципи.  \\n3. Борщу из грибов: сначала обжимай глубокий миска грыбной массы, добиваясь ее настойчивой текстуры. Потом следуют основные ингредиенты, а гренки можно додавать в конце, чтобы не испортить текст. Этот вариант является более легким и менее пикантным.   \\n4. Соргоборщ: в качестве основы используется сорг, которое можно купить в магазинах или подготавить самостоятельно, смешав кукурузное зерно с мукой. Оно дает борщину более сладкую, менестреличную текстовую структуру, что делает ей более мягкой и нежной.    \\n5. Супер-легкая борш: для этого варенье используют карта"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_gptq_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
