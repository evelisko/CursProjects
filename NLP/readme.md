# Чат-бот для телеграм.

## Описание.

Чат-бот умеет обучен на датасете рецептов и датасете диалогов. Так что он ведет беседу от имени персонажа - "Кибер-бабушки".

В чат-боте также имеется проверка текста на токсичность. 
Если пользователь в диалоге с ботом "позволяет себе лишнего", то используются заготовленные фразы, в которых бот просит пользователя не "не выражаться".


## Запуск бота.

1. Загрузить модели по ссылке - 
2. Модели должны быть расположены в корневой папке NLP/models.
3. Создать бота для телеграмм @BotFather. Получить токен и добавить его в поле `token` в файле config/config.ini.
4. Запустить бота: 
```
python -m main
```
#### Меню бота
* `/temperature` - Открывает меню для изменения температуры генерации. Меню позволяет задать температуру в диапазоне от 0.1 до 0.9.
* `/rag_mode` - Открывает меню для включения/выключения режима, при котором модель при ответе на вопросы пользователя использует базу данных. Этот подход называется - Retrieval-Augmented Generation.


## Подготовка данных.

Датасет для с рецептами - 
https://www.kaggle.com/coolonce/recipes-and-interpretation-dim
Диалоги Датасет с диалогами - "IlyaGusev/gpt_roleplay_realm"

Папка с датасетом для обучения модели и финальная модель можно получить по ссылке - https://disk.yandex.ru/d/ANtqBesTJrgFMQ


## Обучение модели.

Обучение модели проходит в два этапа.
1. Обучение модели по инструкции.
2. Обучение модели на диалогах.

На первом этапе модель учится выдавать ответы на запросы о приготовлении того или иного блюда.
На втором - вести диалог от имени персонажа, Кибер-бабушки. На данном этапе модель учится соблюдать в диалоге стиль общения персонажа.

## Подготовка датасета.
### Подготовка датасета с рецептами.
1. Датасет с рецептами - https://www.kaggle.com/coolonce/recipes-and-interpretation-dim
2. Скачать с kaggle с датасетом `all_recepies_inter.csv.zip` и разархивировать его в папку `train`.
3. Выполнить код из ноутбука - `train/notebooks/receptes_dataset_prepere.ipynb`

В результате должен появиться каталог train/datasets с 6-ю файлами в формате jsonl. 

### Подготовка датасета с диалогами.
Датасет с диалогами персонажей взят с HuggingFace - "IlyaGusev/gpt_roleplay_realm".

Выполнить код из ноутбука - `train/notebooks/dialoges_dataset_prepere.ipynb`

> Примечание: Данный датасет не совсем подходит для обучения модели. 
Для обучения модели, в датасете комментарии пользователя и бота должны чередоваться. В данном датаесте это не так. Иногда ответ модели разбит на два сообщения. Это необходимо поправить вручную.

>Подготовленные датасеты расположены в папке datasets на - https://disk.yandex.ru/d/ANtqBesTJrgFMQ
 

## Обучение модели.

### Обучение диалоговой LLM-модели.
Для экономии вычислительных ресурсов обучение не исходной модели, а адаптера LoRa.
Обучение модели выполнялось на GPU A100 c 40GB видео-памяти.

Базовая модель - `TheBloke/Llama-2-7B-fp16`.

Данная модель не очень хорошо подходит для русского языка. 
Пошел на небольшую хитрость. Выполнил дообучение уже существующего LoRa - адаптера `IlyaGusev/saiga2_7b_lora` обученного на этой модели.

Оба этапа обучения модели описаны в файле `train/train_adapter.ipynb`

Проверить модель можно с помощью `test_lora.ipynb`

За основу скриптов для обучения моделей взяты скрипты из https://github.com/IlyaGusev/rulm 

> Выводы: В плане переноса стиля лучше всего себя показала модель полученная на 35-м шаге. Она запомнила ключевые фразы, используемые персонажем во время общения. 
Но, что касается рецептов блюд, то здесь модель дает иногда очень странные рекомендации.

### Обучение модели для поиска рецептов по ключевым словам в базе данных.



