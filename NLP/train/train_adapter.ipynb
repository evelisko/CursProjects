{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dshome/anaconda3/envs/auto_gptq_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer, GenerationConfig, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "from peft import PeftConfig, PeftModel\n",
    "from train.train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m src.train --config-file configs/saicuna_13b.json --train-file train.jsonl --val-file val.jsonl  --output-dir models/directum_13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = 'models/granny_model'\n",
    "adapter_check_point = f'{adapter_path}/checkpoint'\n",
    "os.makedirs(adapter_path, exist_ok=True)\n",
    "os.makedirs(adapter_check_point, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  32000\n",
      "PAD:  0 <unk>\n",
      "BOS:  1 <s>\n",
      "EOS:  2 </s>\n",
      "UNK:  0 <unk>\n",
      "SEP:  1 <s>\n",
      "{'messages': [{'role': 'user', 'content': 'Тартар из\\xa0тунца в\\xa0авокадо'}, {'role': 'bot', 'content': '1. Нарежьте филе тунца очень мелкими кусочками.\\r\\n2. Нарежьте киви.\\r\\n3. Для того чтобы приготовить трюфельный майонез, очень мелко нарежьте черный трюфель и смешайте с домашним майонезом и соком из половины лайма.\\r\\n4. В миске аккуратно перемешайте тунец и киви с трюфельным майонезом.\\r\\n5. Тонкими ломтиками нарежьте авокадо. Поместите нахлестом их на коврик для приготовления роллов.\\r\\n6. Распределите тунец посередине авокадо и аккуратно заверните в виде ролла.\\r\\n7. Разрежьте получившийся ролл пополам, украсьте сверху красной икрой, и можно подавать.'}], 'source': 'alpaca'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 49/20913 [00:00<00:42, 487.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1788, 13, 30041, 29982, 448, 6746, 7279, 29899, 30031, 29910, 3378, 13785, 29892, 6961, 29899, 576, 1782, 29932, 29892, 6961, 29899, 1093, 3176, 3506, 29892, 1778, 29972, 6798, 2370, 3807, 11453, 29892, 18421, 27618, 507, 1413, 23321, 24398, 18432, 3828, 24090, 9610, 2430, 4724, 3378, 2237, 29951, 29892, 1710, 20940, 5413, 1077, 1782, 676, 29892, 1447, 20176, 676, 606, 2394, 10260, 3501, 490, 1695, 19300, 24846, 490, 1382, 29935, 2430, 836, 2583, 29957, 29889, 1703, 29982, 6253, 551, 1155, 531, 2942, 5752, 29982, 989, 11425, 15953, 26344, 29892, 22327, 840, 2359, 2950, 1805, 20302, 1604, 4449, 2100, 1499, 1413, 9648, 4734, 19074, 6122, 606, 863, 1802, 29975, 3501, 3862, 9430, 490, 1864, 28287, 606, 1084, 20002, 29959, 29889, 1703, 29982, 27876, 477, 14752, 19943, 531, 6331, 6326, 989, 29309, 2019, 989, 606, 531, 1345, 1802, 2321, 30005, 2263, 644, 7489, 3862, 9430, 614, 8384, 4816, 29889, 2, 29871, 13, 1, 1404, 13, 30041, 1515, 11741, 1866, 30081, 1500, 29921, 3432, 490, 30081, 29910, 984, 642, 1802, 2, 29871, 13, 1, 9225, 13, 29896, 29889, 2372, 587, 29998, 29978, 730, 5406, 753, 12998, 29921, 3432, 614, 23963, 2508, 7574, 989, 6226, 2223, 10546, 989, 22993, 13, 29906, 29889, 2372, 587, 29998, 29978, 730, 8094, 1221, 22993, 13, 29941, 29889, 22418, 11453, 18421, 1695, 588, 29932, 1928, 1413, 1937, 17087, 8765, 24178, 18063, 16214, 29972, 29892, 614, 23963, 2508, 29944, 551, 665, 587, 29998, 29978, 730, 13392, 2370, 1937, 17087, 8765, 693, 606, 531, 1488, 4604, 29977, 730, 531, 19794, 30002, 5472, 18063, 16214, 15438, 606, 1778, 4751, 1866, 28072, 933, 11046, 29977, 1155, 22993, 13, 29946, 29889, 939, 4157, 11106, 20356, 1382, 494, 13050, 2942, 1488, 4604, 29977, 730, 12998, 27720, 606, 8094, 1221, 531, 1937, 17087, 8765, 693, 4470, 18063, 16214, 15438, 22993, 13, 29945, 29889, 1703, 2031, 21456, 29871, 7792, 811, 13021, 665, 587, 29998, 29978, 730, 1097, 984, 642, 1802, 29889, 2195, 1488, 1415, 730, 665, 29988, 753, 23298, 11244, 665, 1186, 516, 641, 29951, 3807, 1695, 19300, 6052, 1561, 29944, 3176, 22993, 13, 29953, 29889, 29355, 16143, 644, 730, 12998, 27720, 10865, 9951, 821, 1097, 984, 642, 1802, 606, 20356, 1382, 494, 13050, 24587, 12897, 490, 21986, 1561, 17502, 22993, 13, 29955, 29889, 26595, 587, 29998, 29978, 730, 12052, 17754, 1198, 1561, 19414, 733, 1268, 684, 29959, 29892, 26993, 1210, 730, 531, 3531, 12735, 6697, 29935, 2082, 606, 29951, 6318, 29892, 606, 26634, 12493, 24058, 29889, 2, 29871, 13, 2]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1, 9225, 13, 29896, 29889, 2372, 587, 29998, 29978, 730, 5406, 753, 12998, 29921, 3432, 614, 23963, 2508, 7574, 989, 6226, 2223, 10546, 989, 22993, 13, 29906, 29889, 2372, 587, 29998, 29978, 730, 8094, 1221, 22993, 13, 29941, 29889, 22418, 11453, 18421, 1695, 588, 29932, 1928, 1413, 1937, 17087, 8765, 24178, 18063, 16214, 29972, 29892, 614, 23963, 2508, 29944, 551, 665, 587, 29998, 29978, 730, 13392, 2370, 1937, 17087, 8765, 693, 606, 531, 1488, 4604, 29977, 730, 531, 19794, 30002, 5472, 18063, 16214, 15438, 606, 1778, 4751, 1866, 28072, 933, 11046, 29977, 1155, 22993, 13, 29946, 29889, 939, 4157, 11106, 20356, 1382, 494, 13050, 2942, 1488, 4604, 29977, 730, 12998, 27720, 606, 8094, 1221, 531, 1937, 17087, 8765, 693, 4470, 18063, 16214, 15438, 22993, 13, 29945, 29889, 1703, 2031, 21456, 29871, 7792, 811, 13021, 665, 587, 29998, 29978, 730, 1097, 984, 642, 1802, 29889, 2195, 1488, 1415, 730, 665, 29988, 753, 23298, 11244, 665, 1186, 516, 641, 29951, 3807, 1695, 19300, 6052, 1561, 29944, 3176, 22993, 13, 29953, 29889, 29355, 16143, 644, 730, 12998, 27720, 10865, 9951, 821, 1097, 984, 642, 1802, 606, 20356, 1382, 494, 13050, 24587, 12897, 490, 21986, 1561, 17502, 22993, 13, 29955, 29889, 26595, 587, 29998, 29978, 730, 12052, 17754, 1198, 1561, 19414, 733, 1268, 684, 29959, 29892, 26993, 1210, 730, 531, 3531, 12735, 6697, 29935, 2082, 606, 29951, 6318, 29892, 606, 26634, 12493, 24058, 29889, 2, 29871, 13, 2]\n",
      "Full prompt: <s> system\n",
      "Ты - Кибер-Бабушка, полу-робот, полу-человек, созданный для того, чтобы сохранить лучшие качества традиционных бабушек, такие как забота, доброта и мастерство в приготовлении вкусных блюд. Ты знакома с передовыми технологиями, благодаря чему может обеспечить безопасность и удобство своим внукам и гостям. Ты способна помочь с любыми вопросами и с радостью делится своим опытом.</s> \n",
      "<s> user\n",
      "Тартар из тунца в авокадо</s> \n",
      "<s> bot\n",
      "1. Нарежьте филе тунца очень мелкими кусочками.\n",
      "2. Нарежьте киви.\n",
      "3. Для того чтобы приготовить трюфельный майонез, очень мелко нарежьте черный трюфель и смешайте с домашним майонезом и соком из половины лайма.\n",
      "4. В миске аккуратно перемешайте тунец и киви с трюфельным майонезом.\n",
      "5. Тонкими ломтиками нарежьте авокадо. Поместите нахлестом их на коврик для приготовления роллов.\n",
      "6. Распределите тунец посередине авокадо и аккуратно заверните в виде ролла.\n",
      "7. Разрежьте получившийся ролл пополам, украсьте сверху красной икрой, и можно подавать.</s> \n",
      "</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20913/20913 [00:30<00:00, 683.25it/s]\n",
      "  2%|▏         | 112/6971 [00:00<00:12, 555.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1788, 13, 30041, 29982, 448, 6746, 7279, 29899, 30031, 29910, 3378, 13785, 29892, 6961, 29899, 576, 1782, 29932, 29892, 6961, 29899, 1093, 3176, 3506, 29892, 1778, 29972, 6798, 2370, 3807, 11453, 29892, 18421, 27618, 507, 1413, 23321, 24398, 18432, 3828, 24090, 9610, 2430, 4724, 3378, 2237, 29951, 29892, 1710, 20940, 5413, 1077, 1782, 676, 29892, 1447, 20176, 676, 606, 2394, 10260, 3501, 490, 1695, 19300, 24846, 490, 1382, 29935, 2430, 836, 2583, 29957, 29889, 1703, 29982, 6253, 551, 1155, 531, 2942, 5752, 29982, 989, 11425, 15953, 26344, 29892, 22327, 840, 2359, 2950, 1805, 20302, 1604, 4449, 2100, 1499, 1413, 9648, 4734, 19074, 6122, 606, 863, 1802, 29975, 3501, 3862, 9430, 490, 1864, 28287, 606, 1084, 20002, 29959, 29889, 1703, 29982, 27876, 477, 14752, 19943, 531, 6331, 6326, 989, 29309, 2019, 989, 606, 531, 1345, 1802, 2321, 30005, 2263, 644, 7489, 3862, 9430, 614, 8384, 4816, 29889, 2, 29871, 13, 1, 1404, 13, 30041, 2031, 20940, 22464, 23018, 19455, 14714, 933, 1778, 30081, 29935, 1488, 676, 2082, 606, 30081, 20142, 2082, 30098, 2, 29871, 13, 1, 9225, 13, 29896, 29889, 939, 4157, 11106, 531, 1488, 4604, 29977, 730, 1604, 29919, 4179, 717, 29892, 14908, 551, 29892, 1778, 693, 29892, 2282, 29977, 3432, 606, 29871, 29896, 7349, 3176, 29744, 13860, 29998, 1382, 3212, 5010, 29969, 4896, 1868, 2721, 29964, 753, 1868, 2394, 12329, 29889, 14120, 576, 10093, 25505, 29975, 3215, 730, 4157, 29951, 1502, 4416, 1447, 17636, 3463, 2082, 2394, 29935, 5551, 29889, 3982, 29951, 6318, 730, 469, 2510, 7337, 606, 1685, 6842, 29978, 730, 490, 6785, 843, 956, 693, 3316, 665, 29871, 29941, 29900, 4157, 18313, 29994, 29896, 12978, 22993, 13, 29906, 29889, 14120, 576, 10093, 3212, 29904, 29969, 10786, 730, 836, 8279, 6627, 531, 7220, 576, 1520, 665, 12831, 14522, 614, 21740, 29889, 6546, 3102, 29942, 29978, 730, 1538, 22451, 10093, 29919, 13142, 22339, 2394, 12329, 606, 5571, 1515, 29978, 730, 2721, 29921, 20940, 836, 8279, 1499, 717, 29892, 2771, 4865, 4184, 17423, 1695, 5508, 570, 733, 29871, 29896, 7349, 29944, 10848, 13860, 29998, 2476, 2399, 1229, 606, 7421, 16143, 1225, 29970, 733, 19759, 29988, 5395, 29892, 1447, 26467, 811, 1578, 588, 3716, 1521, 676, 531, 1604, 29919, 15139, 7349, 6887, 29871, 29906, 29994, 29941, 4157, 1864, 3327, 29889, 2195, 23842, 4647, 28023, 1447, 3102, 7593, 29977, 730, 1694, 2402, 1538, 29959, 1868, 2394, 12329, 22993, 13, 29941, 29889, 2195, 840, 846, 29977, 730, 1778, 531, 1488, 676, 2082, 606, 13392, 2082, 606, 29951, 6318, 29889, 2, 29871, 13, 2]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1, 9225, 13, 29896, 29889, 939, 4157, 11106, 531, 1488, 4604, 29977, 730, 1604, 29919, 4179, 717, 29892, 14908, 551, 29892, 1778, 693, 29892, 2282, 29977, 3432, 606, 29871, 29896, 7349, 3176, 29744, 13860, 29998, 1382, 3212, 5010, 29969, 4896, 1868, 2721, 29964, 753, 1868, 2394, 12329, 29889, 14120, 576, 10093, 25505, 29975, 3215, 730, 4157, 29951, 1502, 4416, 1447, 17636, 3463, 2082, 2394, 29935, 5551, 29889, 3982, 29951, 6318, 730, 469, 2510, 7337, 606, 1685, 6842, 29978, 730, 490, 6785, 843, 956, 693, 3316, 665, 29871, 29941, 29900, 4157, 18313, 29994, 29896, 12978, 22993, 13, 29906, 29889, 14120, 576, 10093, 3212, 29904, 29969, 10786, 730, 836, 8279, 6627, 531, 7220, 576, 1520, 665, 12831, 14522, 614, 21740, 29889, 6546, 3102, 29942, 29978, 730, 1538, 22451, 10093, 29919, 13142, 22339, 2394, 12329, 606, 5571, 1515, 29978, 730, 2721, 29921, 20940, 836, 8279, 1499, 717, 29892, 2771, 4865, 4184, 17423, 1695, 5508, 570, 733, 29871, 29896, 7349, 29944, 10848, 13860, 29998, 2476, 2399, 1229, 606, 7421, 16143, 1225, 29970, 733, 19759, 29988, 5395, 29892, 1447, 26467, 811, 1578, 588, 3716, 1521, 676, 531, 1604, 29919, 15139, 7349, 6887, 29871, 29906, 29994, 29941, 4157, 1864, 3327, 29889, 2195, 23842, 4647, 28023, 1447, 3102, 7593, 29977, 730, 1694, 2402, 1538, 29959, 1868, 2394, 12329, 22993, 13, 29941, 29889, 2195, 840, 846, 29977, 730, 1778, 531, 1488, 676, 2082, 606, 13392, 2082, 606, 29951, 6318, 29889, 2, 29871, 13, 2]\n",
      "Full prompt: <s> system\n",
      "Ты - Кибер-Бабушка, полу-робот, полу-человек, созданный для того, чтобы сохранить лучшие качества традиционных бабушек, такие как забота, доброта и мастерство в приготовлении вкусных блюд. Ты знакома с передовыми технологиями, благодаря чему может обеспечить безопасность и удобство своим внукам и гостям. Ты способна помочь с любыми вопросами и с радостью делится своим опытом.</s> \n",
      "<s> user\n",
      "Тонкие гречневые блины со сметаной и черной…</s> \n",
      "<s> bot\n",
      "1. В миске смешайте обе муки, молоко, соль, яйца и 1 столовую ложку размягченного топленого масла. Хорошо взбейте миксером до однородной массы. Закройте пленкой и отправьте в холодильник на 30 минут–1 час.\n",
      "2. Хорошо разогрейте блинную сковороду на среднем огне. Добавьте небольшое количество масла и жарьте тонкие блинчики, выкладывая примерно по 1 столовой ложке теста и распределяя по поверхности, до золотистого цвета с обеих сторон 2–3 минуты. По необходимости добавляйте еще немного масла.\n",
      "3. Подавайте со сметаной и черной икрой.</s> \n",
      "</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6971/6971 [00:10<00:00, 661.30it/s]\n",
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` attribute will be overwritten with the one you passed to `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_IDS\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            1,  1788,    13, 30041, 29982,   448,  6746,  7279, 29899, 30031,\n",
      "        29910,  3378, 13785, 29892,  6961, 29899,   576,  1782, 29932, 29892,\n",
      "         6961, 29899,  1093,  3176,  3506, 29892,  1778, 29972,  6798,  2370,\n",
      "         3807, 11453, 29892, 18421, 27618,   507,  1413, 23321, 24398, 18432,\n",
      "         3828, 24090,  9610,  2430,  4724,  3378,  2237, 29951, 29892,  1710,\n",
      "        20940,  5413,  1077,  1782,   676, 29892,  1447, 20176,   676,   606,\n",
      "         2394, 10260,  3501,   490,  1695, 19300, 24846,   490,  1382, 29935,\n",
      "         2430,   836,  2583, 29957, 29889,  1703, 29982,  6253,   551,  1155,\n",
      "          531,  2942,  5752, 29982,   989, 11425, 15953, 26344, 29892, 22327,\n",
      "          840,  2359,  2950,  1805, 20302,  1604,  4449,  2100,  1499,  1413,\n",
      "         9648,  4734, 19074,  6122,   606,   863,  1802, 29975,  3501,  3862,\n",
      "         9430,   490,  1864, 28287,   606,  1084, 20002, 29959, 29889,  1703,\n",
      "        29982, 27876,   477, 14752, 19943,   531,  6331,  6326,   989, 29309,\n",
      "         2019,   989,   606,   531,  1345,  1802,  2321, 30005,  2263,   644,\n",
      "         7489,  3862,  9430,   614,  8384,  4816, 29889,     2, 29871,    13,\n",
      "            1,  1404,    13, 30041,  1515, 11741,  1866, 30081,  1500, 29921,\n",
      "         3432,   490, 30081, 29910,   984,   642,  1802,     2, 29871,    13,\n",
      "            1,  9225,    13, 29896, 29889,  2372,   587, 29998, 29978,   730,\n",
      "         5406,   753, 12998, 29921,  3432,   614, 23963,  2508,  7574,   989,\n",
      "         6226,  2223, 10546,   989, 22993,    13, 29906, 29889,  2372,   587,\n",
      "        29998, 29978,   730,  8094,  1221, 22993,    13, 29941, 29889, 22418,\n",
      "        11453, 18421,  1695,   588, 29932,  1928,  1413,  1937, 17087,  8765,\n",
      "        24178, 18063, 16214, 29972, 29892,   614, 23963,  2508, 29944,   551,\n",
      "          665,   587, 29998, 29978,   730, 13392,  2370,  1937, 17087,  8765,\n",
      "          693,   606,   531,  1488,  4604, 29977,   730,   531, 19794, 30002,\n",
      "         5472, 18063, 16214, 15438,   606,  1778,  4751,  1866, 28072,   933,\n",
      "        11046, 29977,  1155, 22993,    13, 29946, 29889,   939,  4157, 11106,\n",
      "        20356,  1382,   494, 13050,  2942,  1488,  4604, 29977,   730, 12998,\n",
      "        27720,   606,  8094,  1221,   531,  1937, 17087,  8765,   693,  4470,\n",
      "        18063, 16214, 15438, 22993,    13, 29945, 29889,  1703,  2031, 21456,\n",
      "        29871,  7792,   811, 13021,   665,   587, 29998, 29978,   730,  1097,\n",
      "          984,   642,  1802, 29889,  2195,  1488,  1415,   730,   665, 29988,\n",
      "          753, 23298, 11244,   665,  1186,   516,   641, 29951,  3807,  1695,\n",
      "        19300,  6052,  1561, 29944,  3176, 22993,    13, 29953, 29889, 29355,\n",
      "        16143,   644,   730, 12998, 27720, 10865,  9951,   821,  1097,   984,\n",
      "          642,  1802,   606, 20356,  1382,   494, 13050, 24587, 12897,   490,\n",
      "        21986,  1561, 17502, 22993,    13, 29955, 29889, 26595,   587, 29998,\n",
      "        29978,   730, 12052, 17754,  1198,  1561, 19414,   733,  1268,   684,\n",
      "        29959, 29892, 26993,  1210,   730,   531,  3531, 12735,  6697, 29935,\n",
      "         2082,   606, 29951,  6318, 29892,   606, 26634, 12493, 24058, 29889,\n",
      "            2, 29871,    13,     2])\n",
      "MASK\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "LABELS\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "            1,  9225,    13, 29896, 29889,  2372,   587, 29998, 29978,   730,\n",
      "         5406,   753, 12998, 29921,  3432,   614, 23963,  2508,  7574,   989,\n",
      "         6226,  2223, 10546,   989, 22993,    13, 29906, 29889,  2372,   587,\n",
      "        29998, 29978,   730,  8094,  1221, 22993,    13, 29941, 29889, 22418,\n",
      "        11453, 18421,  1695,   588, 29932,  1928,  1413,  1937, 17087,  8765,\n",
      "        24178, 18063, 16214, 29972, 29892,   614, 23963,  2508, 29944,   551,\n",
      "          665,   587, 29998, 29978,   730, 13392,  2370,  1937, 17087,  8765,\n",
      "          693,   606,   531,  1488,  4604, 29977,   730,   531, 19794, 30002,\n",
      "         5472, 18063, 16214, 15438,   606,  1778,  4751,  1866, 28072,   933,\n",
      "        11046, 29977,  1155, 22993,    13, 29946, 29889,   939,  4157, 11106,\n",
      "        20356,  1382,   494, 13050,  2942,  1488,  4604, 29977,   730, 12998,\n",
      "        27720,   606,  8094,  1221,   531,  1937, 17087,  8765,   693,  4470,\n",
      "        18063, 16214, 15438, 22993,    13, 29945, 29889,  1703,  2031, 21456,\n",
      "        29871,  7792,   811, 13021,   665,   587, 29998, 29978,   730,  1097,\n",
      "          984,   642,  1802, 29889,  2195,  1488,  1415,   730,   665, 29988,\n",
      "          753, 23298, 11244,   665,  1186,   516,   641, 29951,  3807,  1695,\n",
      "        19300,  6052,  1561, 29944,  3176, 22993,    13, 29953, 29889, 29355,\n",
      "        16143,   644,   730, 12998, 27720, 10865,  9951,   821,  1097,   984,\n",
      "          642,  1802,   606, 20356,  1382,   494, 13050, 24587, 12897,   490,\n",
      "        21986,  1561, 17502, 22993,    13, 29955, 29889, 26595,   587, 29998,\n",
      "        29978,   730, 12052, 17754,  1198,  1561, 19414,   733,  1268,   684,\n",
      "        29959, 29892, 26993,  1210,   730,   531,  3531, 12735,  6697, 29935,\n",
      "         2082,   606, 29951,  6318, 29892,   606, 26634, 12493, 24058, 29889,\n",
      "            2, 29871,    13,     2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/815 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/dshome/anaconda3/envs/auto_gptq_env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  0%|          | 2/815 [07:00<47:39:45, 211.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0926, 'learning_rate': 1.1999999999999999e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/815 [10:33<47:43:33, 211.59s/it]"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 11.76 GiB total capacity; 8.87 GiB already allocated; 216.38 MiB free; 9.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/dshome/Documents/GeekBrains/CursProjects/NLP/train_adapter.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dshome/Documents/GeekBrains/CursProjects/NLP/train_adapter.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(config_file\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain/train_config.json\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dshome/Documents/GeekBrains/CursProjects/NLP/train_adapter.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m       train_file\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdatasets/train_receptes.jsonl\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dshome/Documents/GeekBrains/CursProjects/NLP/train_adapter.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m       val_file\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdatasets/test_receptes.jsonl\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dshome/Documents/GeekBrains/CursProjects/NLP/train_adapter.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m       \u001b[39m# checkpoint=adapter_check_point,\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dshome/Documents/GeekBrains/CursProjects/NLP/train_adapter.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m       output_dir\u001b[39m=\u001b[39;49madapter_path)\n",
      "File \u001b[0;32m~/Documents/GeekBrains/CursProjects/NLP/train/train.py:281\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config_file, train_file, val_file, output_dir, model_path, adapter_path, checkpoint, sample_rate, report_to, seed, use_flash_attention_2)\u001b[0m\n\u001b[1;32m    269\u001b[0m trainer \u001b[39m=\u001b[39m TrainerNoBaseSave(\n\u001b[1;32m    270\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    271\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m     data_collator\u001b[39m=\u001b[39mdata_collator\n\u001b[1;32m    276\u001b[0m )\n\u001b[1;32m    278\u001b[0m \u001b[39m# if trainer_config.get(\"report_to\", \"wandb\") == \"wandb\":\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39m#     wandb.init(project=\"rulm_self_instruct\", name=config_file)\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(checkpoint)\n\u001b[1;32m    282\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39msave model to \u001b[39m\u001b[39m{\u001b[39;00moutput_dir\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    283\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_gptq_env/lib/python3.10/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1596\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_gptq_env/lib/python3.10/site-packages/transformers/trainer.py:1892\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1889\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1891\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1892\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1894\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1895\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1896\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1897\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1898\u001b[0m ):\n\u001b[1;32m   1899\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_gptq_env/lib/python3.10/site-packages/transformers/trainer.py:2787\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   2786\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2787\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m   2789\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_gptq_env/lib/python3.10/site-packages/accelerate/accelerator.py:1985\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1983\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1984\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1985\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_gptq_env/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_gptq_env/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_gptq_env/lib/python3.10/site-packages/torch/autograd/function.py:274\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    273\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 274\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_gptq_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:157\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(outputs_with_grad) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    154\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnone of output has requires_grad=True,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m this checkpoint() is not necessary\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 157\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(outputs_with_grad, args_with_grad)\n\u001b[1;32m    158\u001b[0m grads \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(inp\u001b[39m.\u001b[39mgrad \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inp, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    159\u001b[0m               \u001b[39mfor\u001b[39;00m inp \u001b[39min\u001b[39;00m detached_inputs)\n\u001b[1;32m    161\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m) \u001b[39m+\u001b[39m grads\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_gptq_env/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 11.76 GiB total capacity; 8.87 GiB already allocated; 216.38 MiB free; 9.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train(config_file='train/train_config.json',\n",
    "      train_file='datasets/train_receptes.jsonl',\n",
    "      val_file='datasets/test_receptes.jsonl',\n",
    "      # checkpoint=adapter_check_point,\n",
    "      output_dir=adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(os.environ.get(\"WORLD_SIZE\", 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = 'models/'\n",
    "adapter_path = 'models/'\n",
    "tockenizer_path = 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tockenizer_path, use_fast=False) #, padding_side='left')\n",
    "# generation_config = GenerationConfig.from_pretrained(model_name, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = PeftConfig.from_pretrained(base_model_path)\n",
    "base_model_config = AutoConfig.from_pretrained(base_model_path)\n",
    "torch_dtype = base_model_config.torch_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        llm_int8_threshold=6.0,\n",
    "        llm_int8_has_fp16_weight=False,\n",
    "        bnb_4bit_compute_dtype=torch_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    ),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(base_model_path,\n",
    "                                                     do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = PeftConfig.from_pretrained(adapter_path)\n",
    "# base_model_config = AutoConfig.from_pretrained(base_model_path)\n",
    "\n",
    "# torch_dtype = base_model_config.torch_dtype\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    load_in_8bit=True,\n",
    "    # quantization_config=quantization_config, # tесли не хотим 4-bit.\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapter_path,\n",
    "    adapter_name=\"adapter1_name\",\n",
    "    torch_dtype=torch_dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.active_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настраиваем конфиги для адаптера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Надо сделать так чтобы разморозились веса\n",
    "# только у адарптера и обучался именно он, а не модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8GB'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{int(torch.cuda.mem_get_info()[0] / 1024 ** 3) - 2}GB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# У меня сегодня тоже возникла задача решить проблему с нехваткой памяти (уже правда в рамках обучения другой нейросети), поэтому полез настраивать device_map и max_memory опции, чтобы тренировка, если места мало, могла залезать в системную оперативную память, вот как сделал:\n",
    "\n",
    "model = model_types[model_type].from_pretrained(\n",
    "  model_name,\n",
    "  load_in_8bit=True,\n",
    "  device_map='auto',\n",
    "  max_memory={0: f'{int(torch.cuda.mem_get_info()[0] / 1024 ** 3) - 2}GB'},\n",
    "  torch_dtype=torch_dtype,\n",
    "  use_flash_attention_2=use_flash_attention_2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_gptq_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
