{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae589075-3eca-42c3-9847-83e6dfd0a601",
   "metadata": {},
   "source": [
    "### Ставим зависимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f543bda-6e67-4abb-a2a4-f1023cb6deb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install jupyterlab_widgets ipywidgets -q\n",
    "!sudo apt-get install git -y\n",
    "!sudo apt-get install git-lfs -y\n",
    "!git clone https://github.com/IlyaGusev/rulm.git\n",
    "!git reset --hard 3bc0cd6700708c84ee444005f9e21c8b36230937\n",
    "!git clean -df\n",
    "!pip install -r ./rulm/requirements.txt -q\n",
    "!pip uninstall wandb -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1054b7f2-fd15-4cf8-9fcc-503194125f1c",
   "metadata": {},
   "source": [
    "### Загружаем базовую модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36dc6f9-2848-4de6-830b-24c110f9de5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd ./rulm/self_instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079845bc-29ba-42fa-9519-ab3d83ed8fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_MODEL_HG_NAME = r'lmsys/vicuna-13b-v1.5'\n",
    "BASE_MODEL_LOCAL_PATH = r'models/vicuna-13b-v1.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcbded7-7dcf-472d-bdb4-c8c7580b7cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=BASE_MODEL_HG_NAME, \n",
    "    local_dir=BASE_MODEL_LOCAL_PATH, \n",
    "    ignore_patterns=[\"LICENSE\", \"README.md\", \"*.safetensors\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5586bb3-8b57-4953-a3f3-ca6ec19a7c44",
   "metadata": {},
   "source": [
    "### Фиксим конфиги токенизатора\n",
    "https://github.com/IlyaGusev/rulm/blob/master/self_instruct/README.md#fix-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d46dc7-6171-459a-9f5c-2b295a4be752",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "replacements = {\n",
    "    \"tokenizer_config.json\": {\n",
    "        \"tokenizer_class\": \"LlamaTokenizer\",\n",
    "        \"model_max_length\": 4096,\n",
    "        \"padding_side\": \"left\",\n",
    "        \"bos_token\": \"<s>\",\n",
    "        \"eos_token\": \"</s>\",\n",
    "        \"pad_token\": \"<unk>\",\n",
    "        \"unk_token\": \"<unk>\",\n",
    "        \"clean_up_tokenization_spaces\": False,\n",
    "        \"special_tokens_map_file\": \"special_tokens_map.json\"\n",
    "    },\n",
    "    \"special_tokens_map.json\": {\n",
    "        \"bos_token\": \"<s>\",\n",
    "        \"eos_token\": \"</s>\",\n",
    "        \"pad_token\": \"<unk>\",\n",
    "        \"unk_token\": \"<unk>\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for filename, new_content in replacements.items():\n",
    "    with open(f'{BASE_MODEL_LOCAL_PATH}/{filename}', 'w', encoding='utf-8') as fp:\n",
    "        json.dump(new_content, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ea2e6-3344-485c-a103-a5e397aa1f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /home/rulm/self_instruct/models/vicuna-13b-v1.5/pytorch_model-00001-of-00003.bin /home/rulm/self_instruct/models/directum_13b/pytorch_model-00001-of-00003.bin\n",
    "!cp /home/rulm/self_instruct/models/vicuna-13b-v1.5/pytorch_model-00002-of-00003.bin /home/rulm/self_instruct/models/directum_13b/pytorch_model-00002-of-00003.bin\n",
    "!cp /home/rulm/self_instruct/models/vicuna-13b-v1.5/pytorch_model-00003-of-00003.bin /home/rulm/self_instruct/models/directum_13b/pytorch_model-00003-of-00003.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4ad041-e644-4f45-b0f7-849598ff6e7d",
   "metadata": {},
   "source": [
    "### Удаляем системный промпт"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e813466a-8704-4544-9767-4ad07aa19608",
   "metadata": {},
   "source": [
    "Без удаления системного промпта модель будет работать только с ним (либо работать плохо без него)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43594219-9dd8-4dd0-8d32-6fcaf830725c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "internal_prompts = {\n",
    "    \"system_prompt\": \"\",\n",
    "    \"system_message_template\": \"\",\n",
    "    \"user_message_template\": \"<s>{role}\\n{content}</s>\\n\",\n",
    "    \"bot_message_template\": \"<s>{role}\\n{content}</s>\\n\",\n",
    "    \"user_role\": \"user\",\n",
    "    \"bot_role\": \"bot\",\n",
    "    \"system_role\": \"system\",\n",
    "    \"suffix\": \"<s>bot\"\n",
    "}\n",
    "\n",
    "with open(f'./internal_prompts/saiga_v2.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump(internal_prompts, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d196bb3d-6fea-4b35-96e4-20689e17539c",
   "metadata": {},
   "source": [
    "### Отключаем wandb и загрузку модели в 8 битах"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d05f9-5412-4e3f-92ab-2db3945ff41c",
   "metadata": {},
   "source": [
    "В файле **rulm/self_instruct/src/train.py** закомментировать строки 6, 245, 246\n",
    "\n",
    "В файле **rulm/self_instruct/src/train.py** на 104 строке изменить значение  **report_to: str = 'wandb'** на **report_to: str = None**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0209f9-81de-4785-b32f-9d8c84ba1199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'./configs/saicuna_13b.json', 'r', encoding='utf-8') as fp:\n",
    "    model_config = json.loads(fp.read())\n",
    "    \n",
    "model_config['load_in_8bit'] = False\n",
    "\n",
    "with open(f'./configs/saicuna_13b.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump(model_config, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964601be-cb1e-43ad-9f29-d012bfba38b1",
   "metadata": {},
   "source": [
    "### Фиксим двойной EOS-токен в конце промпта обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b55ad48-3bd7-4f2f-a500-e20a3f609c5d",
   "metadata": {},
   "source": [
    "В файле **rulm/self_instruct/src/dataset.py** на 34 строке изменить значение **self.add_global_eos** на **False**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f310304-c915-4d73-8fa6-8903d4ef7f1c",
   "metadata": {},
   "source": [
    "В файле **rulm/self_instruct/src/dataset.py** заменить цикл\n",
    "\n",
    "```\n",
    "for message, role in conversation.iter_messages():\n",
    "    message_input_ids = self.get_tokens(message)\n",
    "    message_labels = message_input_ids\n",
    "    if len(input_ids) + len(message_input_ids) > self.max_tokens_count:\n",
    "        break\n",
    "\n",
    "    labels_mask = [self.labels_pad_token_id for _ in range(len(message_input_ids))]\n",
    "    if role != conversation.bot_role and self.only_target_loss:\n",
    "        message_labels = labels_mask\n",
    "\n",
    "    input_ids.extend(message_input_ids)\n",
    "    labels.extend(message_labels)\n",
    "```\n",
    "\n",
    "на\n",
    "\n",
    "```\n",
    "conv = []\n",
    "        \n",
    "for message, role in conversation.iter_messages():\n",
    "    conv.append([role, message])\n",
    "\n",
    "conv[-1][1] = conv[-1][1].strip()\n",
    "\n",
    "for msg in conv:\n",
    "    message_input_ids = self.get_tokens(msg[1])\n",
    "    message_labels = message_input_ids\n",
    "    if len(input_ids) + len(message_input_ids) > self.max_tokens_count:\n",
    "        break\n",
    "\n",
    "    labels_mask = [self.labels_pad_token_id for _ in range(len(message_input_ids))]\n",
    "    if msg[0] != conversation.bot_role and self.only_target_loss:\n",
    "        message_labels = labels_mask\n",
    "\n",
    "    input_ids.extend(message_input_ids)\n",
    "    labels.extend(message_labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af1c465-8915-4b7b-a743-5e16eef6d33f",
   "metadata": {},
   "source": [
    "### Загрузка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1df0a1-892c-42b2-bd95-ad48313f5788",
   "metadata": {},
   "source": [
    "Загрузи **train.jsonl** и **val.jsonl** в **/rulm/self_instruct**\n",
    "\n",
    "Формат датасета (каждая беседа на новой строке, согласно формату jsonl):\n",
    "\n",
    "```\n",
    "{\"messages\": [{\"role\": \"user\", \"content\": \"Как дела?\"}, {\"role\": \"bot\", \"content\": \"Отлично\"}], \"source\": \"alpaca\"}\n",
    "{\"messages\": [{\"role\": \"user\", \"content\": \"Кто ты?\"}, {\"role\": \"bot\", \"content\": \"Я бот\"}], \"source\": \"alpaca\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f7b53d-71da-4bb8-bd8d-6dbd7eabfce4",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f97085-3565-4bfc-9ff4-37b421ef2ce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m src.train --config-file configs/saicuna_13b.json --train-file train.jsonl --val-file val.jsonl  --output-dir models/directum_13b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd518958-10b0-41a4-98d8-6cce540539fe",
   "metadata": {},
   "source": [
    "### Исправляем конфиг инференса обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe43e5-9c31-45f1-8cad-ff27688fcb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./models/vicuna-13b-v1.5/generation_config.json  ./models/directum_13b/generation_config.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672fa6c8-cdd5-4597-8bc7-3a46f93395f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./models/vicuna-13b-v1.5/generation_config.json', 'w') as fp:\n",
    "    json.dump({\n",
    "        \"pad_token_id\": 0,\n",
    "        \"bos_token_id\": 1,\n",
    "        \"eos_token_id\": 2,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9,\n",
    "        \"top_k\": 40,\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\": 2560,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"no_repeat_ngram_size\": 15,\n",
    "    }, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f619e4",
   "metadata": {},
   "source": [
    "### Сливаем адаптер в модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f81e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_LOCAL_PATH,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='cuda:0',\n",
    "    local_files_only=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    './models/directum_13b',\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='cuda:0',\n",
    "    local_files_only=False\n",
    ")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained('merged_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
